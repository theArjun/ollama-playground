// ğŸ¦™ Ollama client imports for chat functionality
use ollama_rs::generation::chat::request::ChatMessageRequest;
use ollama_rs::generation::chat::ChatMessage;
use ollama_rs::Ollama;
// ğŸ“¦ Serialization tools for data exchange
use serde::{Deserialize, Serialize};
// ğŸŒ‰ Tauri framework imports for IPC and state management
use tauri::ipc::Channel;
use tauri::State;
// ğŸ”’ Async synchronization primitives
use tokio::sync::Mutex;
// ğŸŒŠ Stream processing utilities
use futures_util::StreamExt;

// ğŸ  Application state container - holds our Ollama client
struct AppState {
    ollama: Mutex<Ollama>, // ğŸ” Thread-safe Ollama client wrapper
}

// ğŸ’¬ Chat request structure from frontend
// Deserialization is important here, since it is required as input in chat command.
#[derive(Serialize, Deserialize)]
struct ChatRequest {
    model: String,              // ğŸ¤– AI model name (e.g., "llama2", "mistral")
    messages: Vec<ChatMessage>, // ğŸ“ Conversation history
}

// ğŸ“¤ Response structure sent back to frontend
#[derive(Serialize)]
struct ChatResponse {
    message: String, // ğŸ’­ AI-generated response content
}

// ğŸ“‹ Command to fetch available local models
#[tauri::command]
async fn get_models(state: State<'_, AppState>) -> Result<Vec<String>, String> {
    let models = {
        // ğŸ”“ Acquire lock on Ollama client
        let client = state.ollama.lock().await;
        // ğŸ” Query local models from Ollama
        client
            .list_local_models()
            .await
            .map_err(|e| format!("Failed to list models: {:?}", e))?
    }; // ğŸ”’ Lock automatically released here

    // ğŸ·ï¸ Extract just the model names from the full model objects
    let model_names = models
        .iter()
        .map(|local_model| local_model.name.clone())
        .collect();

    Ok(model_names)
}

// ğŸš€ Main chat command - handles streaming AI responses
#[tauri::command]
async fn chat_to_llm(
    request: ChatRequest,             // ğŸ“¥ Incoming chat request
    on_stream: Channel<ChatResponse>, // ğŸ“¡ Streaming channel to frontend
    state: State<'_, AppState>,       // ğŸ  Application state
) -> Result<(), String> {
    // ğŸ”“ Get exclusive access to Ollama client
    let client = state.ollama.lock().await;
    // ğŸ› ï¸ Build chat request for Ollama API
    let chat_request = ChatMessageRequest::new(request.model, request.messages);

    // ğŸŒŠ Initialize streaming response from Ollama
    let mut stream = client
        .send_chat_messages_stream(chat_request)
        .await
        .map_err(|e| format!("{:?}", e))?;

    // ğŸ”„ Process each chunk of the streaming response
    // to run next() method, it requires StreamExt;
    while let Some(stream_response) = stream.next().await {
        // âš ï¸ Handle potential stream errors
        let response = stream_response.map_err(|e| format!("Stream error {:?}", e))?;
        // ğŸ“¦ Package response for frontend
        let chat_response = ChatResponse {
            message: response.message.content,
        };

        // ğŸ“¤ Send response chunk to frontend via IPC channel
        on_stream.send(chat_response).map_err(|e| e.to_string())?;
    }

    Ok(()) // âœ… Stream completed successfully
}

// ğŸš€ Application entry point
#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
    tauri::Builder::default()
        .plugin(tauri_plugin_opener::init()) // ğŸ”Œ Enable file/URL opening capabilities
        .manage(AppState {
            // ğŸ—ï¸ Initialize application state
            ollama: Mutex::new(Ollama::default()), // ğŸ¦™ Create default Ollama client
        })
        .invoke_handler(tauri::generate_handler![get_models, chat_to_llm]) // ğŸ¯ Register command handlers
        .run(tauri::generate_context!()) // ğŸƒ Start the application
        .expect("error while running tauri application"); // ğŸ’¥ Panic on startup failure
}
